<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description"
    content="We introduce LargeSceneModel, which utilizes two unposed and uncalibrated images as input, and reconstructs the explicit radiance field, encompassing geometry, appearance, and semantics in real-time.">
  <meta property="og:title" content="Large Spatial Model: Real-time Unposed Images to Semantic 3D" />
  <meta property="og:description" content="Novel view synthesis via feed-forward 3D Gaussian inference from two images." />
  <!-- <meta property="og:url" content="https://pixelsplat.github.io/" />
  <meta property="og:image" content="https://pixelsplat.github.io/static/images/banner.png" /> -->
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />

  <meta name="twitter:title" content="Large Spatial Model: Real-time Unposed Images to Semantic 3D">/
  <meta name="twitter:description" content="Novel view synthesis via feed-forward 3D Gaussian inference from two images.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <!-- <meta name="twitter:image" content="https://pixelsplat.github.io/static/images/banner.png"> -->
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="NeRF, novel view synthesis, 3D Gaussians">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Large Spatial Model</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">

  <link rel="stylesheet" href="static/css/bootstrap.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <!-- <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script> -->
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bootstrap.bundle.min.js"></script>
</head>

<style>
  .before,
  .after {
    margin: 0;
  }

  .summary-box {
  background-color: #fdd28e; /* Light blue background */
  /* border-left: 5px solid #2b8af7; Dark blue left border */
  padding: 20px;
  margin: 10px 0;
  font-family: Arial, sans-serif;
  border-radius: 5px; /* Rounds the corners */
  border: 2px solid #FAFCB8; /* Fine orange border */
  }

  p {
      margin: 0;
      color: #333; /* Dark text for better readability */
  }
  .before figcaption,
  .after figcaption {
    background: #fff;
    border: 1px solid #c0c0c0;
    border-radius: 4px;
    color: #2e3452;
    opacity: 0.8;
    padding: 4px;
    position: absolute;
    top: 50%;
    transform: translateY(-50%);
    line-height: 70%;
  }

  .before figcaption {
    left: 4px;
  }

  .after figcaption {
    right: 4px;
  }
</style>


<body>
  <div class="container">
    <!-- Title -->
      <h1 class="pt-5 title"><span style="color: #ff7512;font-weight: bolder;">L</span>arge<span style="color: #ff7512;font-weight: bolder;">S</span>patial<span style="color: #ff7512;font-weight: bolder;">M</span>odel: End-to-end Unposed Images to Semantic 3D</h1>
    <!-- <div class="d-flex flex-row justify-content-center">
      <a>Anonymous Authors</a>
    </div> -->
    <div class="mb-2" style="font-size: medium; text-align: center;">
      <span class="author-block">
        <a href="https://zhiwenfan.github.io/">Zhiwen Fan</a><sup>1,2*</sup></span>,&nbsp;
      <span class="author-block">
        <a href="https://largespatialmodel.github.io/">Jian Zhang</a><sup>3*</sup></span>,&nbsp;
      <span class="author-block">
        <a href="https://wenyancong.com/">Wenyan Cong</a><sup>1</sup></span>,&nbsp;
      <span class="author-block">
        <a href="https://peihaowang.github.io/">Peihao Wang</a><sup>1</sup></span>,&nbsp;
      <span class="author-block">
        <a href="https://largespatialmodel.github.io/">Renjie Li</a><sup>4</sup></span>,&nbsp;
      <span class="author-block">
        <a href="https://x.com/KairunWen">Kairun Wen</a><sup>3</sup></span>,&nbsp;
      <span class="author-block">
        <a href="https://shijiezhou-ucla.github.io/#-services">Shijie Zhou</a><sup>5</sup></span>,&nbsp;
      <span class="author-block">
        <a href="https://www.ee.ucla.edu/achuta-kadambi/">Achuta Kadambi</a><sup>5</sup></span>,&nbsp;
      <span class="author-block">
        <a href="https://vita-group.github.io/research.html">Zhangyang Wang</a><sup>1</sup></span>,&nbsp;
      <span class="author-block">
        <a href="https://faculty.cc.gatech.edu/~danfei/">Danfei Xu</a><sup>2,6</sup></span>,&nbsp;
      <span class="author-block">
        <a href="https://www.borisivanovic.com/">Boris Ivanovic</a><sup>2</sup></span>,&nbsp;
      <span class="author-block">
        <a href="https://web.stanford.edu/~pavone/">Marco Pavone</a><sup>2,7</sup></span>,&nbsp;
      <span class="author-block">
        <a href="https://yuewang.xyz/">Yue Wang</a><sup>2,8</sup></span>
    </div>
    <div class="mb-4" style="text-align: center;">
      <span><sup>1</sup>UT Austin</span>,&nbsp;
      <span><sup>2</sup>Nvidia Research</span>,&nbsp;
      <span><sup>3</sup>XMU</span>,&nbsp;
      <span><sup>4</sup>TAMU</span>,&nbsp;<br>
      <span><sup>5</sup>UCLA</span>,&nbsp;
      <span><sup>6</sup>GaTech</span>,&nbsp;
      <span><sup>7</sup>Stanford University</span>,&nbsp;
      <span><sup>8</sup>USC</span><br>
      <span class="univerity-block" style="color: rgb(255, 117, 18); font-weight: bolder;">NeurIPS 2024</span>
    </div>

    <div class="w-100 d-flex flex-row justify-content-center mt-4 gap-2">
      <!-- Paper PDF -->
      <a href="static/NeurIPS2024_Large_Spaital_Model_arxiv.pdf" target="_blank" class="btn btn-dark" role="button">
        <span class="icon">
          <i class="ai ai-arxiv"></i>
        </span>
        <span>Paper</span>
      </a>
    </div> 

    <!-- Teaser -->
    <div class="w-100 my-4">
      <!-- <img src="static/images/teaser.png" class="img-fluid w-100 mt-2 mb-3" alt="comparison on ACID dataset" /> -->
      <img src="static/images/largespatialmodel.svg" class="img-fluid w-100 mt-2 mb-3" alt="comparison on ACID dataset" />
      <!-- <video class="w-100 d-block" autoplay controls muted loop>
        <source src="static/videos/teaser.mp4" type="video/mp4">
      </video> -->
      <div class="summary-box tldr mb-4">
      <!-- <div class="alert alert-primary tldr mb-4"> -->
        <strong>TL;DR</strong>: LSM utilizes two unposed and uncalibrated images as input, and reconstructs the explicit radiance field, encompassing geometry, appearance, and semantics in real-time.
      </div>
    </div>



    <!-- </section> -->
    <div class="columns is-centered has-text-centered">

    <br/>
    <!-- Demo -->
    <h2 class="title is-3 has-text-centered"><span style="color: #f06d15 ;font-weight: bolder;">Demo</span></h2>
    <div class="content has-text-justified">
      <video id="matting-video" autoplay controls muted playsinline width="100%">
        <source src="static/videos/demo1.mp4"
        type="video/mp4">
      </video>
      

    <!-- Abstract -->
    <br><br/>
    <h2 class="title is-3 has-text-centered"><span style="color: #f06d15 ;font-weight: bolder;">Abstract</span></h2>
    <div class="content has-text-justified">
      <p>
      <!-- </div><p class="mb-4"> -->
        Reconstructing and understanding 3D structures from a limited number of im-
ages is a classical problem in computer vision. Traditional approaches typically
decompose this task into multiple subtasks, involving several stages of complex
mappings between different data representations. For example, dense reconstruc-
tion using Structure-from-Motion (SfM) requires transforming images into key
points, optimizing camera parameters, and estimating structures. Following this,
accurate sparse reconstructions are necessary for further dense modeling, which is
then input into task-specific neural networks. This multi-stage paradigm leads to
significant processing times and engineering complexity
        </p>
        <br>
        <p>
          In this work, we introduce the <em>Large Spatial Model</em> (<strong>LSM</strong>), which directly pro-
          cesses unposed RGB images into semantic radiance fields. LSM simultaneously
          estimates geometry, appearance, and semantics in a single feed-forward pass and
          can synthesize versatile label maps by interacting through language at novel views.
          Built on a general Transformer-based framework, LSM integrates global geometry
          via pixel-aligned point maps. To improve spatial attribute regression, we adopt
          local context aggregation with multi-scale fusion, enhancing the accuracy of fine
          local details. To address the scarcity of labeled 3D semantic data and enable natural
          language-driven scene manipulation, we incorporate a pre-trained 2D language-
          based segmentation model into a 3D-consistent semantic feature field. An efficient
          decoder parameterizes a set of semantic anisotropic Gaussians, allowing supervised
          end-to-end learning. Comprehensive experiments on various tasks demonstrate
          that LSM unifies multiple 3D vision tasks directly from unposed images, achieving
          real-time semantic 3D reconstruction for the first time.
      </p>
    

    <!-- Method -->
    <br>
    <h2 class="title is-3 has-text-centered"><span style="color: #f06d15 ;font-weight: bolder;">Method Overview</span></h2>
    
    <div class="content has-text-justified">
      <img src="static/images/method_overview.png" class="img-fluid w-100 mt-2 mb-3" alt="comparison on ACID dataset" />
      <p>
        Our method utilizes input images from which pixel-aligned point maps are regressed using a generic Transformer. Point-based scene parameters are then predicted employing another Transformer that facilitates local context aggregation and hierarchical fusion. The model elevate 2D pre-trained feature to facilate consistent 3D feature field. It is supervised end-to-end, minimizing the loss function through comparisons against ground truth and rasterized feature maps on new views. During the inference stage, our approach is capable of predicting the scene representation without requiring camera parameters, enabling real-time semantic 3D reconstruction.<br> <br>
      </p>
      </div>


    
    <!-- Comparisons -->
    <h2 class="title is-3 has-text-centered"><span style="color: #f06d15 ;font-weight: bolder;">Scalable Visualized Results</span></h2>
    <div class="content has-text-justified">
      <video id="matting-video" autoplay controls muted playsinline width="100%">
        <source src="static/videos/demo2.mp4"
        type="video/mp4">
      </video>

  

    <!-- Footer -->
    <footer class="border-top mt-5 py-4">
      <!-- This page's code uses elements from this <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
        target="_blank">Academic Project Page
        Template</a>.
    </footer>
  </div> -->
</body>

</html>